# -*- coding: utf-8 -*-
import os
import csv
from io import BytesIO
nltk.download("punkt_tab", quiet=True)


import numpy as np
import pandas as pd
import streamlit as st
from PyPDF2 import PdfReader
from docx import Document

import nltk
from nltk import word_tokenize, pos_tag

# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆëœ€)
nltk.download("punkt", quiet=True)
nltk.download("averaged_perceptron_tagger", quiet=True)

# ---------- ê¸°ë³¸ ì„¤ì • ---------- #
POS_CATEGORIES = {"Verb": "VB", "Noun": "NN", "Adjective": "JJ", "Adverb": "RB"}

ACADEMIC_WORDS = {
    "analyze", "approach", "area", "assess", "assume", "authority", "concept",
    "consistent", "constitute", "context", "contract", "create", "data",
    "definition", "derive", "distribute", "economy", "environment", "establish",
    "estimate", "evidence", "export", "factor", "formula", "function", "identify",
    "income", "indicate", "interpret", "involve", "issue", "legal", "major",
    "method", "occur", "percent", "policy", "principle", "process", "require",
    "research", "response", "role", "section", "sector", "significant", "similar",
    "source", "specific", "structure", "theory", "vary"
}


# ---------- PDF â†’ DOCX + í…ìŠ¤íŠ¸ ì¶”ì¶œ ---------- #
def pdf_to_docx_and_text(pdf_file) -> tuple[BytesIO, str]:
    """
    Streamlitì˜ ì—…ë¡œë“œ íŒŒì¼ ê°ì²´ë¥¼ ë°›ì•„ì„œ
    1) PDF í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ DOCXë¡œ ì €ì¥í•˜ê³ 
    2) ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜í•œë‹¤.
    (ì›ë³¸ pdf_to_docx_simpleê³¼ ê°€ëŠ¥í•œ í•œ ë™ì¼í•œ ë¡œì§)
    """
    reader = PdfReader(pdf_file)
    doc = Document()
    all_text_parts = []

    pages = reader.pages
    num_pages = len(pages)

    for i, page in enumerate(pages):
        text = page.extract_text()
        if text:
            all_text_parts.append(text)
            for line in text.splitlines():
                doc.add_paragraph(line)
        # ë§ˆì§€ë§‰ í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ page break
        if i < num_pages - 1:
            doc.add_page_break()

    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)

    full_text = "\n".join(all_text_parts)
    return buffer, full_text


# ---------- ë¶„ì„ í•¨ìˆ˜ (ì›ë³¸ ì•Œê³ ë¦¬ì¦˜ ìœ ì§€) ---------- #
def extract_pos(text, prefix):
    tagged = pos_tag(word_tokenize(text))
    return [w.lower() for w, tag in tagged if tag.startswith(prefix)]


def calculate_mattr(words, win=50):
    if not words:
        return 0.0
    if len(words) < win:
        return len(set(words)) / len(words)
    return float(
        np.mean(
            [len(set(words[i:i + win])) / win for i in range(len(words) - win + 1)]
        )
    )


def calculate_category_mattr(cat, allw, win=11):
    if len(allw) < win:
        return len(set(cat)) / len(cat) if cat else 0.0
    vals = []
    for i in range(len(allw) - win + 1):
        window = allw[i:i + win]
        hits = [w for w in window if w in cat]
        if hits:
            vals.append(len(set(hits)) / win)
    return float(np.mean(vals)) if vals else 0.0


def calc_lexical_soph(allw):
    """AWL ratio + bigram/trigram type-token ratio"""
    if not allw:
        return 0.0, 0.0, 0.0
    total = len(allw)
    awl = sum(1 for w in allw if w in ACADEMIC_WORDS) / total
    bigr = ["_".join(allw[i:i + 2]) for i in range(len(allw) - 1)]
    trigr = ["_".join(allw[i:i + 3]) for i in range(len(allw) - 2)]
    big = len(set(bigr)) / len(bigr) if bigr else 0.0
    tri = len(set(trigr)) / len(trigr) if trigr else 0.0
    return round(awl, 4), round(big, 4), round(tri, 4)


def analyze_text(filename: str, text: str, win_all: int, win_pos: int) -> dict:
    """í•œ íŒŒì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•´ MATTR + LexSoph ê³„ì‚°"""
    tokens = word_tokenize(text)
    allw = [w.lower() for w in tokens if w.isalpha()]

    row = {
        "Filename": filename,
        "All_words_MATTR": round(calculate_mattr(allw, win_all), 4),
    }

    for lbl, pref in POS_CATEGORIES.items():
        cat = extract_pos(text, pref)
        row[f"{lbl}_MATTR"] = round(
            calculate_category_mattr(cat, allw, win_pos), 4
        )

    awl, big, tri = calc_lexical_soph(allw)
    row["LexSoph_AWLratio"] = awl
    row["LexSoph_BigramRatio"] = big
    row["LexSoph_TrigramRatio"] = tri

    return row


# ---------- Streamlit ë©”ì¸ ì•± ---------- #
def main():
    st.set_page_config(
        page_title="ë””ì§€í„¸ë§ë­‰ì¹˜ ë¶„ì„ê¸° (PDF â†’ DOCX + MATTR/LexSoph)",
        page_icon="ğŸ“š",
        layout="wide",
    )

    st.title("ğŸ“š ë””ì§€í„¸ë§ë­‰ì¹˜ ë¶„ì„ê³¼ ì–¸ì–´êµìœ¡ ë¶„ì„ê¸° (Streamlit ë²„ì „)")
    st.markdown(
        """
        1. **PDF íŒŒì¼ë“¤ì„ ì—…ë¡œë“œ**í•˜ë©´, ê° íŒŒì¼ì„ Word(DOCX)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  
        2. ë™ì‹œì— PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¡œ **MATTR + Lexical sophistication** ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  
        3. ì›ë³¸ Tkinter/ttkbootstrap ë²„ì „ê³¼ **ë™ì¼í•œ ìˆ˜ì‹ê³¼ window size ë¡œì§**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
    )

    # ì‚¬ì´ë“œë°”: window size ì„¤ì •
    st.sidebar.header("Window size ì„¤ì •")
    win_all = st.sidebar.number_input(
        "All words window size", min_value=5, max_value=500, value=50, step=1
    )
    win_pos = st.sidebar.number_input(
        "POS window size", min_value=5, max_value=200, value=11, step=1
    )

    st.sidebar.markdown("---")
    st.sidebar.info("ì—¬ëŸ¬ ê°œì˜ PDFë¥¼ í•œ ë²ˆì— ì˜¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    uploaded_files = st.file_uploader(
        "ë¶„ì„í•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
        type=["pdf"],
        accept_multiple_files=True,
    )

    if st.button("PDF ë³€í™˜ + ë¶„ì„ ì‹œì‘"):
        if not uploaded_files:
            st.warning("ë¨¼ì € PDF íŒŒì¼ì„ í•˜ë‚˜ ì´ìƒ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            st.stop()

        results = []
        docx_downloads = []

        progress = st.progress(0)
        status_text = st.empty()

        for idx, up in enumerate(uploaded_files, start=1):
            status_text.text(f"{idx}/{len(uploaded_files)} ì²˜ë¦¬ ì¤‘: {up.name}")
            try:
                # PDF â†’ DOCX + í…ìŠ¤íŠ¸ ì¶”ì¶œ
                docx_bytes, text = pdf_to_docx_and_text(up)

                # ë¶„ì„
                row = analyze_text(up.name, text, int(win_all), int(win_pos))
                results.append(row)

                # DOCX ë‹¤ìš´ë¡œë“œìš© ë°ì´í„° ì €ì¥
                docx_downloads.append((up.name, docx_bytes))

            except Exception as e:
                st.error(f"âŒ {up.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

            progress.progress(idx / len(uploaded_files))

        status_text.text("ì™„ë£Œ!")

        if not results:
            st.error("ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
        df_results = pd.DataFrame(results)
        st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼ (MATTR + Lexical sophistication)")
        st.dataframe(df_results, use_container_width=True)

        # CSV ë‹¤ìš´ë¡œë“œ
        csv_buf = BytesIO()
        df_results.to_csv(csv_buf, index=False, encoding="utf-8-sig")
        csv_buf.seek(0)
        st.download_button(
            label="ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_buf,
            file_name="results.csv",
            mime="text/csv",
        )

        st.markdown("---")
        st.subheader("ğŸ“„ ë³€í™˜ëœ DOCX íŒŒì¼ ë‹¤ìš´ë¡œë“œ")

        for name, b in docx_downloads:
            base = os.path.splitext(os.path.basename(name))[0]
            st.download_button(
                label=f"{base}.docx ë‹¤ìš´ë¡œë“œ",
                data=b,
                file_name=f"{base}.docx",
                mime=(
                    "application/vnd.openxmlformats-officedocument."
                    "wordprocessingml.document"
                ),
            )


if __name__ == "__main__":
    main()
