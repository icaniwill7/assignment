# app.py
import os
import io
import csv
from datetime import datetime

import streamlit as st
import numpy as np
import pandas as pd
from nltk import word_tokenize, pos_tag, download
from docx import Document
from PyPDF2 import PdfReader
from nltk import word_tokenize, pos_tag, download

# ---------- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ---------- #
download("punkt", quiet=True)
download("punkt_tab", quiet=True)          # ğŸ”¹ ì´ ì¤„ ì¶”ê°€
download("averaged_perceptron_tagger", quiet=True)

# ---------- ìƒìˆ˜ ì„¤ì • ---------- #
POS_CATEGORIES = {
    "Verb": "VB",
    "Noun": "NN",
    "Adjective": "JJ",
    "Adverb": "RB",
}

ACADEMIC_WORDS = {
    "analyze", "approach", "area", "assess", "assume", "authority", "concept",
    "consistent", "constitute", "context", "contract", "create", "data",
    "definition", "derive", "distribute", "economy", "environment",
    "establish", "estimate", "evidence", "export", "factor", "formula",
    "function", "identify", "income", "indicate", "interpret", "involve",
    "issue", "legal", "major", "method", "occur", "percent", "policy",
    "principle", "process", "require", "research", "response", "role",
    "section", "sector", "significant", "similar", "source", "specific",
    "structure", "theory", "vary",
}

# ---------- ë¶„ì„ìš© í•¨ìˆ˜ ---------- #
def extract_pos(text: str, prefix: str):
    """íŠ¹ì • í’ˆì‚¬(POS prefix)ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
    tokens = word_tokenize(text)
    tagged = pos_tag(tokens)
    return [w.lower() for w, tag in tagged if tag.startswith(prefix)]


def calculate_mattr(words, window_size=50):
    """ì „ì²´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ MATTR ê³„ì‚°"""
    if not words:
        return 0.0
    n = len(words)
    if n < window_size:
        return len(set(words)) / n
    ratios = []
    for i in range(n - window_size + 1):
        window = words[i : i + window_size]
        ratios.append(len(set(window)) / window_size)
    return float(np.mean(ratios))


def calculate_category_mattr(category_words, all_words, window_size=11):
    """ì „ì²´ ë‹¨ì–´ ì‹œí€€ìŠ¤ ì•ˆì—ì„œ íŠ¹ì • category ë‹¨ì–´ë“¤ì˜ MATTR ë¹„ìŠ·í•˜ê²Œ ê³„ì‚°"""
    if not all_words:
        return 0.0
    n = len(all_words)
    if n < window_size:
        return len(set(category_words)) / len(category_words) if category_words else 0.0

    # ìœˆë„ìš°ë§ˆë‹¤ category ë‹¨ì–´ë§Œ í•„í„°ë§í•´ì„œ type/token ë¹„ìœ¨ì„ ê³„ì‚°
    ratios = []
    cat_set = set(category_words)
    for i in range(n - window_size + 1):
        window = all_words[i : i + window_size]
        hits = [w for w in window if w in cat_set]
        if hits:
            ratios.append(len(set(hits)) / window_size)
    return float(np.mean(ratios)) if ratios else 0.0


def calc_lexical_soph(all_words):
    """AWL ë¹„ìœ¨, bigram type ë¹„ìœ¨, trigram type ë¹„ìœ¨"""
    if not all_words:
        return 0.0, 0.0, 0.0

    total = len(all_words)
    awl_ratio = sum(1 for w in all_words if w in ACADEMIC_WORDS) / total

    bigrams = ["_".join(all_words[i : i + 2]) for i in range(len(all_words) - 1)]
    trigrams = ["_".join(all_words[i : i + 3]) for i in range(len(all_words) - 2)]

    bigram_ratio = len(set(bigrams)) / len(bigrams) if bigrams else 0.0
    trigram_ratio = len(set(trigrams)) / len(trigrams) if trigrams else 0.0

    return round(awl_ratio, 4), round(bigram_ratio, 4), round(trigram_ratio, 4)


# ---------- PDF â†’ DOCX + í…ìŠ¤íŠ¸ ì¶”ì¶œ ---------- #
def pdf_to_docx_and_text(uploaded_file):
    """Streamlit ì—…ë¡œë“œëœ PDF íŒŒì¼ 1ê°œë¥¼ DOCX + ì „ì²´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    reader = PdfReader(uploaded_file)
    doc = Document()

    all_text_parts = []

    for page in reader.pages:
        text = page.extract_text() or ""
        all_text_parts.append(text)
        for line in text.splitlines():
            doc.add_paragraph(line)
        doc.add_page_break()

    full_text = "\n".join(all_text_parts)

    # DOCXë¥¼ ë©”ëª¨ë¦¬ ë²„í¼ì— ì €ì¥
    buf = io.BytesIO()
    doc.save(buf)
    buf.seek(0)

    return buf, full_text


# ---------- í…ìŠ¤íŠ¸ ë¶„ì„ ---------- #
def analyze_text(filename, text, win_all, win_pos):
    tokens = word_tokenize(text)
    all_words = [w.lower() for w in tokens if w.isalpha()]

    row = {"Filename": filename}
    row["All_words_MATTR"] = round(calculate_mattr(all_words, win_all), 4)

    for label, prefix in POS_CATEGORIES.items():
        cat_words = extract_pos(text, prefix)
        row[f"{label}_MATTR"] = round(
            calculate_category_mattr(cat_words, all_words, win_pos), 4
        )

    awl, big, tri = calc_lexical_soph(all_words)
    row["LexSoph_AWLratio"] = awl
    row["LexSoph_BigramRatio"] = big
    row["LexSoph_TrigramRatio"] = tri

    return row


# ---------- Streamlit ë©”ì¸ ---------- #
def main():
    st.set_page_config(
        page_title="ë””ì§€í„¸ë§ë­‰ì¹˜ ë¶„ì„ê¸° (PDF â†’ DOCX + MATTR/LexSoph)",
        page_icon="ğŸ“š",
        layout="wide",
    )

    # ---- ìƒë‹¨ ì œëª© ì˜ì—­ ---- #
    st.title("ë””ì§€í„¸ë§ë­‰ì¹˜ ë¶„ì„ê³¼ ì–¸ì–´êµìœ¡ ë¶„ì„ê¸° (Streamlit ë²„ì „)")
    st.caption("PDF â†’ DOCX ë³€í™˜ + MATTR, Lexical sophistication ë¶„ì„ ìë™í™” ë„êµ¬")

    st.markdown(
        """
        **ì‚¬ìš© ë°©ë²•**
        1. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ *window size*ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.  
        2. ì•„ë˜ ì˜ì—­ì— ë¶„ì„í•  **PDF íŒŒì¼**ë“¤ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.  
        3. `PDF ë³€í™˜ + ë¶„ì„ ì‹œì‘` ë²„íŠ¼ì„ ëˆ„ë¥´ë©´,  
           - ê° PDFê°€ Word(DOCX)ë¡œ ë³€í™˜ë˜ê³   
           - í…ìŠ¤íŠ¸ë¥¼ ì´ìš©í•´ MATTR + Lexical sophistication ì§€í‘œê°€ ê³„ì‚°ë©ë‹ˆë‹¤.
        """,
    )

    st.divider()

    # ---- ì‚¬ì´ë“œë°”: window size ---- #
    st.sidebar.header("âš™ï¸ Window size ì„¤ì •")
    win_all = st.sidebar.number_input(
        "All words window size", min_value=5, max_value=500, value=50, step=1
    )
    win_pos = st.sidebar.number_input(
        "POS window size", min_value=5, max_value=200, value=11, step=1
    )
    st.sidebar.info("â€» ìˆ˜ì—… ì˜ˆì œ ê¸°ì¤€: All words=50, POS=11")

    # ---- íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­ ---- #
    st.subheader("ğŸ“‚ ë¶„ì„í•  PDF íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "ì—¬ëŸ¬ ê°œì˜ PDFë¥¼ í•œ ë²ˆì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        type=["pdf"],
        accept_multiple_files=True,
    )

    col_btn, col_msg = st.columns([1, 3])
    with col_btn:
        start = st.button("ğŸš€ PDF ë³€í™˜ + ë¶„ì„ ì‹œì‘", use_container_width=True)
    with col_msg:
        if uploaded_files:
            st.success(f"{len(uploaded_files)}ê°œì˜ PDFê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("í˜„ì¬ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    if not start:
        return

    if not uploaded_files:
        st.warning("ë¨¼ì € PDF íŒŒì¼ì„ í•˜ë‚˜ ì´ìƒ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    # ---- ì‹¤ì œ ì²˜ë¦¬ ---- #
    results = []
    docx_downloads = []

    progress = st.progress(0)
    status = st.empty()
    total = len(uploaded_files)

    for i, up in enumerate(uploaded_files, start=1):
        status.text(f"{i}/{total} ì²˜ë¦¬ ì¤‘â€¦  ({up.name})")
        try:
            docx_bytes, text = pdf_to_docx_and_text(up)
            row = analyze_text(up.name, text, int(win_all), int(win_pos))
            results.append(row)

            docx_downloads.append((up.name, docx_bytes))
        except Exception as e:
            st.error(f"âŒ {up.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        progress.progress(i / total)

    status.text("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")

    if not results:
        st.error("ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ---- ê²°ê³¼ í…Œì´ë¸” ---- #
    st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼ (MATTR + Lexical sophistication)")
    df = pd.DataFrame(results)
    st.dataframe(df, use_container_width=True)

    # CSV ë‹¤ìš´ë¡œë“œ
    csv_buf = io.StringIO()
    df.to_csv(csv_buf, index=False, encoding="utf-8-sig")
    csv_bytes = csv_buf.getvalue().encode("utf-8-sig")

    st.download_button(
        "ğŸ“¥ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ (results.csv)",
        data=csv_bytes,
        file_name="results.csv",
        mime="text/csv",
    )

    # ---- DOCX ë‹¤ìš´ë¡œë“œ ---- #
    st.divider()
    st.subheader("ğŸ“„ ë³€í™˜ëœ DOCX íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
    for name, buf in docx_downloads:
        base = os.path.splitext(os.path.basename(name))[0]
        st.download_button(
            label=f"ğŸ“„ {base}.docx ë‹¤ìš´ë¡œë“œ",
            data=buf,
            file_name=f"{base}.docx",
            mime=(
                "application/vnd.openxmlformats-officedocument."
                "wordprocessingml.document"
            ),
        )


if __name__ == "__main__":
    main()
