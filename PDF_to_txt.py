# -*- coding: utf-8 -*-
import os
from io import BytesIO

import numpy as np
import pandas as pd
import streamlit as st
from PyPDF2 import PdfReader
from docx import Document

# ğŸ”¥ NLTKëŠ” ë°˜ë“œì‹œ ìµœìƒë‹¨ì—ì„œ import
import nltk
from nltk import word_tokenize, pos_tag

# ğŸ”¥ NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (requirementsì—ì„œ nltk==3.8.1ì´ë©´ ì´ ë‘ ê°œë©´ ì¶©ë¶„)
nltk.download("punkt", quiet=True)
nltk.download("averaged_perceptron_tagger", quiet=True)

# ---------- ê¸°ë³¸ ì„¤ì • (ì›ë³¸ Tk ì½”ë“œì™€ ë™ì¼) ---------- #
POS_CATEGORIES = {
    "Verb": "VB",
    "Noun": "NN",
    "Adjective": "JJ",
    "Adverb": "RB",
}

ACADEMIC_WORDS = {
    "analyze", "approach", "area", "assess", "assume", "authority", "concept",
    "consistent", "constitute", "context", "contract", "create", "data",
    "definition", "derive", "distribute", "economy", "environment", "establish",
    "estimate", "evidence", "export", "factor", "formula", "function", "identify",
    "income", "indicate", "interpret", "involve", "issue", "legal", "major",
    "method", "occur", "percent", "policy", "principle", "process", "require",
    "research", "response", "role", "section", "sector", "significant", "similar",
    "source", "specific", "structure", "theory", "vary"
}


# ---------- PDF â†’ DOCX + í…ìŠ¤íŠ¸ ì¶”ì¶œ ---------- #
def pdf_to_docx_and_text(pdf_file) -> tuple[BytesIO, str]:
    """
    Streamlit ì—…ë¡œë“œ ê°ì²´ (pdf_file)ë¥¼ ë°›ì•„ì„œ
    1) í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ DOCXë¡œ ë³€í™˜í•˜ê³ 
    2) ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜í•œë‹¤.
    (ì›ë˜ pdf_to_docx_simpleê³¼ ê±°ì˜ ë™ì¼í•œ ë¡œì§)
    """
    reader = PdfReader(pdf_file)
    doc = Document()
    all_text_parts = []

    pages = reader.pages
    num_pages = len(pages)

    for i, page in enumerate(pages):
        text = page.extract_text()
        if text:
            all_text_parts.append(text)
            # ì¤„ ë‹¨ìœ„ë¡œ paragraph ì¶”ê°€ (ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°)
            for line in text.splitlines():
                doc.add_paragraph(line)
        # ë§ˆì§€ë§‰ í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ page break ì¶”ê°€
        if i < num_pages - 1:
            doc.add_page_break()

    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)

    full_text = "\n".join(all_text_parts)
    return buffer, full_text


# ---------- ë¶„ì„ í•¨ìˆ˜ë“¤ (ì›ë³¸ Tk ì½”ë“œ ì•Œê³ ë¦¬ì¦˜ ê·¸ëŒ€ë¡œ) ---------- #
def extract_pos(text: str, prefix: str):
    tagged = pos_tag(word_tokenize(text))
    return [w.lower() for w, tag in tagged if tag.startswith(prefix)]


def calculate_mattr(words, win: int = 50) -> float:
    if not words:
        return 0.0
    if len(words) < win:
        return len(set(words)) / len(words)
    return float(
        np.mean(
            [len(set(words[i:i + win])) / win for i in range(len(words) - win + 1)]
        )
    )


def calculate_category_mattr(cat, allw, win: int = 11) -> float:
    if len(allw) < win:
        return len(set(cat)) / len(cat) if cat else 0.0
    vals = []
    for i in range(len(allw) - win + 1):
        window = allw[i:i + win]
        hits = [w for w in window if w in cat]
        if hits:
            vals.append(len(set(hits)) / win)
    return float(np.mean(vals)) if vals else 0.0


def calc_lexical_soph(allw):
    """
    AWL ë¹„ìœ¨ + bigram/trigram type-token ratio
    (ì›ë³¸ ì½”ë“œ calc_lexical_sophì™€ ë™ì¼í•œ ì•„ì´ë””ì–´)
    """
    if not allw:
        return 0.0, 0.0, 0.0

    total = len(allw)
    awl = sum(1 for w in allw if w in ACADEMIC_WORDS) / total

    bigr = ["_".join(allw[i:i + 2]) for i in range(len(allw) - 1)]
    trigr = ["_".join(allw[i:i + 3]) for i in range(len(allw) - 2)]

    big = len(set(bigr)) / len(bigr) if bigr else 0.0
    tri = len(set(trigr)) / len(trigr) if trigr else 0.0

    return round(awl, 4), round(big, 4), round(tri, 4)


def analyze_text(filename: str, text: str, win_all: int, win_pos: int) -> dict:
    """
    í•œ íŒŒì¼(í…ìŠ¤íŠ¸)ì— ëŒ€í•´:
    - All_words_MATTR
    - POSë³„ MATTR
    - Lexical Sophistication
    ê³„ì‚°í•´ì„œ dictë¡œ ë°˜í™˜
    """
    tokens = word_tokenize(text)
    allw = [w.lower() for w in tokens if w.isalpha()]

    row = {
        "Filename": filename,
        "All_words_MATTR": round(calculate_mattr(allw, win_all), 4),
    }

    for lbl, pref in POS_CATEGORIES.items():
        cat = extract_pos(text, pref)
        row[f"{lbl}_MATTR"] = round(
            calculate_category_mattr(cat, allw, win_pos), 4
        )

    awl, big, tri = calc_lexical_soph(allw)
    row["LexSoph_AWLratio"] = awl
    row["LexSoph_BigramRatio"] = big
    row["LexSoph_TrigramRatio"] = tri

    return row


# ---------- Streamlit ë©”ì¸ ì•± ---------- #
def main():
    st.set_page_config(
        page_title="ë””ì§€í„¸ë§ë­‰ì¹˜ ë¶„ì„ê¸° (PDF â†’ DOCX + MATTR/LexSoph)",
        page_icon="ğŸ“š",
        layout="wide",
    )

    st.title("ğŸ“š ë””ì§€í„¸ë§ë­‰ì¹˜ ë¶„ì„ê³¼ ì–¸ì–´êµìœ¡ ë¶„ì„ê¸° (Streamlit ë²„ì „)")
    st.markdown(
        """
        1. **PDF íŒŒì¼ë“¤ì„ ì—…ë¡œë“œ**í•˜ë©´, ê° íŒŒì¼ì„ Word(DOCX)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  
        2. ë™ì‹œì— PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë¡œ **MATTR + Lexical sophistication** ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  
        3. ì›ë˜ Tkinter + ttkbootstrap ë²„ì „ê³¼ **ë™ì¼í•œ ê³„ì‚° ë¡œì§(window size, POS, AWL, n-gram ë¹„ìœ¨)**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
    )

    # ì‚¬ì´ë“œë°”: window size ì„¤ì • (ì›ë˜ ê¸°ë³¸ê°’ 50 / 11 ê·¸ëŒ€ë¡œ)
    st.sidebar.header("Window size ì„¤ì •")
    win_all = st.sidebar.number_input(
        "All words window size", min_value=5, max_value=500, value=50, step=1
    )
    win_pos = st.sidebar.number_input(
        "POS window size", min_value=5, max_value=200, value=11, step=1
    )

    st.sidebar.markdown("---")
    st.sidebar.info("ì—¬ëŸ¬ ê°œì˜ PDFë¥¼ í•œ ë²ˆì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    uploaded_files = st.file_uploader(
        "ë¶„ì„í•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
        type=["pdf"],
        accept_multiple_files=True,
    )

    if st.button("PDF ë³€í™˜ + ë¶„ì„ ì‹œì‘"):
        if not uploaded_files:
            st.warning("ë¨¼ì € PDF íŒŒì¼ì„ í•˜ë‚˜ ì´ìƒ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            st.stop()

        results = []
        docx_downloads = []

        progress = st.progress(0)
        status_text = st.empty()

        total = len(uploaded_files)

        for idx, up in enumerate(uploaded_files, start=1):
            status_text.text(f"{idx}/{total} ì²˜ë¦¬ ì¤‘: {up.name}")
            try:
                # PDF â†’ DOCX + í…ìŠ¤íŠ¸ ì¶”ì¶œ
                docx_bytes, text = pdf_to_docx_and_text(up)

                # ë¶„ì„
                row = analyze_text(up.name, text, int(win_all), int(win_pos))
                results.append(row)

                # DOCX ë‹¤ìš´ë¡œë“œìš© ë°ì´í„° ì €ì¥
                docx_downloads.append((up.name, docx_bytes))

            except Exception as e:
                st.error(f"âŒ {up.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

            progress.progress(idx / total)

        status_text.text("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")

        if not results:
            st.error("ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
        df_results = pd.DataFrame(results)
        st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼ (MATTR + Lexical Sophistication)")
        st.dataframe(df_results, use_container_width=True)

        # CSV ë‹¤ìš´ë¡œë“œ
        csv_buf = BytesIO()
        df_results.to_csv(csv_buf, index=False, encoding="utf-8-sig")
        csv_buf.seek(0)
        st.download_button(
            label="ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ (results.csv)",
            data=csv_buf,
            file_name="results.csv",
            mime="text/csv",
        )

        st.markdown("---")
        st.subheader("ğŸ“„ ë³€í™˜ëœ DOCX íŒŒì¼ ë‹¤ìš´ë¡œë“œ")

        for name, b in docx_downloads:
            base = os.path.splitext(os.path.basename(name))[0]
            st.download_button(
                label=f"{base}.docx ë‹¤ìš´ë¡œë“œ",
                data=b,
                file_name=f"{base}.docx",
                mime=(
                    "application/vnd.openxmlformats-officedocument."
                    "wordprocessingml.document"
                ),
            )


if __name__ == "__main__":
    main()
